name: embedder

resources:
  cloud: gcp
  accelerators: L4:1
  use_spot: true
  cpus: 2+
  disk_size: 50
  ports: 8082

service:
  readiness_probe:
    path: /health
    initial_delay_seconds: 600

  replica_policy: # All required for autoscaling
    min_replicas: 1
    max_replicas: 3
    qps_upper_threshold: 20
    qps_lower_threshold: 10

envs:
  MODEL: BAAI/llm-embedder

run: |
  docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8082:80 --pull always --shm-size 1g \
    ghcr.io/huggingface/text-embeddings-inference:0.6 \
      --model-id $MODEL
