{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b034ff-d4be-4f9e-b545-307cf44eeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aria2c https://huggingface.co/datasets/psmathur/orca_minis_uncensored_dataset/resolve/main/wizardlm_alpaca_dolly_orca_uncensored.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c31c701-de50-4c29-8691-1b79a391a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv 38e3ca794a2e0fc9a7bf51bced5fbc1296c910ac0e0359272e1b9ea282dbb0a1 ../cache_data/wizardlm_alpaca_dolly_orca_uncensored.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1703b6-ae16-4689-b708-dc24a0fe4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "import jsonlines as jsonl\n",
    "from datasets import Dataset\n",
    "\n",
    "fn = \"../cache_data/wizardlm_alpaca_dolly_orca_uncensored.jsonl\"\n",
    "with jsonl.open(fn) as reader:\n",
    "    def from_reader():\n",
    "        for item in reader:\n",
    "            yield {\"input\": \"\", **item}\n",
    "\n",
    "    dataset = Dataset.from_generator(from_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c038de18-f4b5-4c72-a028-0a82e659a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-3877f5370df68f7a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(104179, 83087)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(dataset.filter(lambda row: not (row[\"input\"].strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b56734e-a325-41e5-a7ed-d631895f43a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-3877f5370df68f7a.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda row: not (row[\"input\"].strip())).remove_columns([\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748825b3-18c1-4a12-9abd-1660d37b7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_chatml = lambda name, role, content: dict(\n",
    "    name=name, role=role, content=content,\n",
    ")\n",
    "\n",
    "system = lambda name, content: make_chatml(\n",
    "    role=\"system\",\n",
    "    name=name,\n",
    "    content=content,\n",
    ")\n",
    "\n",
    "situation = lambda content: system(name=\"situation\", content=content)\n",
    "thought = lambda content: system(name=\"thought\", content=content)\n",
    "information = lambda content: system(name=\"information\", content=content)\n",
    "me = lambda content, name=None: make_chatml(\n",
    "    role=\"assistant\",\n",
    "    content=content,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "person = lambda content, name=None: make_chatml(\n",
    "    role=\"user\",\n",
    "    content=content,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "def entry_to_chatml(row):\n",
    "    instruction = row[\"instruction\"]\n",
    "    system = row[\"system\"]\n",
    "    output = row[\"output\"]\n",
    "    \n",
    "    # Start preparing chatml\n",
    "    situation_content = (\n",
    "        \"User is talking to an AI Large Language Model.\"\n",
    "        \"The AI must follow the instructions given below to the letter and help the User with their request.\"\n",
    "    )\n",
    "    \n",
    "    chatml = [\n",
    "        situation(situation_content),\n",
    "        information(f\"Instructions:\\n\\n{system}\"),\n",
    "        person(instruction, name=\"User\"),\n",
    "        me(output, name=\"AI\"),\n",
    "    ]\n",
    "\n",
    "    return dict(chatml=chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af12ec3-f143-4187-b9ac-92d87b4ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-30ef3b0f32417419.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(entry_to_chatml).remove_columns(['instruction', 'system', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec1d243-275a-4139-a277-557bdcd1318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-e1ec1a573d53820b.arrow\n"
     ]
    }
   ],
   "source": [
    "assistant_me_map = {\n",
    "    \"user\": \"person\",\n",
    "    \"assistant\": \"me\",\n",
    "}\n",
    "\n",
    "def make_sections(messages: list[dict]) -> str:\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<|im_start|>\"\n",
    "\n",
    "    result = bos_token + (eos_token+'\\n'+bos_token).join([\n",
    "        (\n",
    "            f\"{message['name']}\"\n",
    "            if message['role'] == 'system' else\n",
    "            f\"{assistant_me_map[message['role']]}{' (' + message['name'] + ')' if message['name'] else ''}\"\n",
    "        )\n",
    "        + f\"\\n{message['content'].strip()}\"\n",
    "        for message in messages\n",
    "    ]) + eos_token\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def transform_to_samantha_dialog(sample):\n",
    "\n",
    "    messages = sample[\"chatml\"]\n",
    "    sample[\"text\"] = make_sections(messages)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(transform_to_samantha_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164fe8b8-5a2b-43ec-9065-6f9cd3264cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"diwank/orca_minis_uncensored-chatml\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac401772-d8fe-4363-9d48-209790254620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>situation\\nUser is talking to an AI Large Language Model.The AI must follow the instructions given below to the letter and help the User with their request.<|im_end|>\\n<|im_start|>information\\nInstructions:\\n\\nYou are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.<|im_end|>\\n<|im_start|>person (User)\\nWhat are the three primary colors?<|im_end|>\\n<|im_start|>me (AI)\\nThe task is asking for the three primary colors. Primary colors are colors that cannot be created by mixing other colors together. The guidelines that we have is that we need to identify the three primary colors. \\n\\nTo find the answer, we need to know that there are two sets of primary colors: additive and subtractive. Additive primary colors are red, green, and blue. These are the primary colors of light and are used in devices such as computer monitors and televisions. Subtractive primary colors are cyan, magenta, and yellow, which are the primary colors of pigments and are used in printing. \\n\\nHowever, the most commonly known primary colors are red, blue, and yellow, which are the primary colors for mixing paints. So, to answer the question, the three primary colors are red, blue, and yellow.<|im_end|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32d9f8a-d9ba-49f5-bfbd-075b2721bd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-ea5387822f79ce9d.arrow\n"
     ]
    }
   ],
   "source": [
    "def to_mpt_fmt(row):\n",
    "    splitter = \"<|im_start|>me (AI)\\n\"\n",
    "    [prompt, response] = row['text'].split(splitter)\n",
    "\n",
    "    prompt += splitter\n",
    "    return dict(prompt=prompt, response=response)\n",
    "\n",
    "mpt_dataset = dataset.map(to_mpt_fmt).remove_columns(['chatml'])\n",
    "# mpt_dataset.push_to_hub(\"diwank/orca_minis_uncensored-mpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816f2c27-2cd8-48b5-906f-52d4cdd02358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/generator/default-a9720ec59ab928d2/0.0.0/cache-49a132e21fe0f9a5_*_of_00040.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'prompt', 'response', 'token_count'],\n",
       "    num_rows: 83087\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_tokens(row):\n",
    "    import math\n",
    "    \n",
    "    text = row['text']\n",
    "\n",
    "    assert \"\\n<|im_start|>me (AI)\\n\" in text\n",
    "\n",
    "    token_count = math.ceil(len(text)/3.6)\n",
    "    return dict(token_count=token_count)\n",
    "\n",
    "mpt_dataset_t = mpt_dataset.map(calc_tokens, num_proc=40)\n",
    "mpt_dataset_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56369af-3592-439a-bb30-a48a938e4356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/83087 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpt_dataset_t = mpt_dataset_t.filter(\n",
    "    lambda x: x[\"token_count\"] < 8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d56e430-c11f-4d5d-b701-05745543b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpt_dataset_t = mpt_dataset_t.remove_columns(['prompt', 'response', 'token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b1c5ee6-41c8-4c79-9578-c66158856e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      9\u001b[0m     o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m mpt_dataset_t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(lens, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      8\u001b[0m lens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      9\u001b[0m     o[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m mpt_dataset_t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(lens, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token_count'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# if using a Jupyter notebook, includue:\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "\n",
    "lens = np.array([\n",
    "    o[\"token_count\"]\n",
    "    for o in mpt_dataset_t[\"train\"]\n",
    "    if o[\"token_count\"] < 5000\n",
    "])\n",
    "\n",
    "plt.hist(lens, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8b0dd8-cebe-4692-970e-a4887aee0475",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mpt_dataset_t \u001b[38;5;241m=\u001b[39m \u001b[43mmpt_dataset_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n\u001b[1;32m      3\u001b[0m mpt_dataset_t\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiwank/orca_minis_uncensored-completion_lm\u001b[39m\u001b[38;5;124m\"\u001b[39m, private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "mpt_dataset_t = mpt_dataset_t.train_test_split(test_size=0.15)\n",
    "\n",
    "mpt_dataset_t.push_to_hub(\"diwank/orca_minis_uncensored-completion_lm\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e19691f1-e7bd-402e-9b1c-2c356bb3424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"julep-ai/beluga-13b-orca_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0692673-05d2-4b82-881d-5fc32ec9ca98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/70622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sep_present(row):\n",
    "    assert \"<|im_start|>me (AI)\\n\" in row[\"text\"]\n",
    "    \n",
    "    sep = tokenizer.encode(\"<|im_start|>me (AI)\\n\", add_special_tokens=False)\n",
    "    input = tokenizer.encode(row[\"text\"])\n",
    "    present = set(sep) <= set(input)\n",
    "\n",
    "    return present\n",
    "\n",
    "check_ds = mpt_dataset_t.filter(sep_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f87b813e-11ed-4214-aabb-6578104dc677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70622, 70622)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpt_dataset_t['train']), len(check_ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d308310-71ed-45fc-9ff2-f140ae78aeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
