{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d6484d-a15a-47dc-8d1f-58d6e9c08869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/yoda/.cache/huggingface/modules/datasets_modules/datasets/openbookqa/f338ccacfbc86fb8c2de3aa1c06d2ce686933de3bca284dba97d32592c52b33f (last modified on Thu Aug  3 10:59:09 2023) since it couldn't be found locally at openbookqa., or remotely on the Hugging Face Hub.\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 1.45M/1.45M [00:02<00:00, 501kB/s]\n",
      "Generating train split: 100%|█████████████████████████████████████████████| 4957/4957 [00:00<00:00, 11439.90 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████| 500/500 [00:00<00:00, 10263.81 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████| 500/500 [00:00<00:00, 10929.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "openbookqa = load_dataset(\"openbookqa\", \"additional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38dc3a8-59f5-446a-9d61-e92228a7fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████████████| 4957/4957 [00:00<00:00, 27737.42 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 36982.24 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 34834.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "openbookqa_subset = openbookqa.filter(lambda row: (\n",
    "    row[\"humanScore\"] >= 1.0\n",
    "    and row[\"clarity\"] >= 1.5\n",
    "    and row[\"question_stem\"].strip().endswith('?')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0a0d23-42bb-40f8-ad08-ccb93c5ac347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['chatml'],\n",
      "        num_rows: 1021\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['chatml'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['chatml'],\n",
      "        num_rows: 106\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "make_chatml = lambda name, role, content: dict(\n",
    "    name=name, role=role, content=content,\n",
    ")\n",
    "\n",
    "system = lambda name, content: make_chatml(\n",
    "    role=\"system\",\n",
    "    name=name,\n",
    "    content=content,\n",
    ")\n",
    "\n",
    "situation = lambda content: system(name=\"situation\", content=content)\n",
    "thought = lambda content: system(name=\"thought\", content=content)\n",
    "information = lambda content: system(name=\"information\", content=content)\n",
    "me = lambda content, name=None: make_chatml(\n",
    "    role=\"assistant\",\n",
    "    content=content,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "person = lambda content, name=None: make_chatml(\n",
    "    role=\"user\",\n",
    "    content=content,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "def get_answer_text(choices, answer_key):\n",
    "    for choice, label in zip(choices[\"text\"], choices[\"label\"]):\n",
    "        if label == answer_key:\n",
    "            return choice\n",
    "\n",
    "def to_chatml(row):\n",
    "    question = row[\"question_stem\"]\n",
    "    choices_data = row[\"choices\"]\n",
    "    fact = row[\"fact1\"]\n",
    "    answer_key = row[\"answerKey\"]\n",
    "\n",
    "    choices = \"\\n\".join([\n",
    "        f\"{label}) {text}\"\n",
    "        for label, text\n",
    "        in zip(choices_data[\"label\"], choices_data[\"text\"])\n",
    "    ])\n",
    "\n",
    "    siutation_text = (\n",
    "        \"A person is talking to an intelligent AI assistant.\"\n",
    "        \" The person is quizzing the AI assistant on miscellaneous questions\"\n",
    "        \" in a multiple choice format.\"\n",
    "        \" The assistant first thinks carefully about the question and lays down its reasoning.\"\n",
    "        \" Then answers the question with the correct choice key and the right answer.\"\n",
    "    )\n",
    "    \n",
    "    mcq = f\"Question: {question}\\nChoices:\\n{choices}\"\n",
    "    thought_text = f\"Reaon: {fact}\"\n",
    "    answer_text = get_answer_text(choices_data, answer_key)\n",
    "    person_text = f\"Question: {question}\\nChoices:\\n{choices}\"\n",
    "    me_text = f\"The correct answer is: {answer_key}. {answer_text}\"\n",
    "    \n",
    "    chatml = [\n",
    "        situation(situation_text),\n",
    "        person(person_text),\n",
    "        thought(thought_text),\n",
    "        me(me_text),\n",
    "    ]\n",
    "    \n",
    "    return dict(chatml=chatml)\n",
    "\n",
    "openbookqa_chatml = openbookqa_subset.map(\n",
    "    to_chatml\n",
    ").remove_columns(['id', 'question_stem', 'choices', 'answerKey', 'fact1', 'humanScore', 'clarity', 'turkIdAnonymized'])\n",
    "print(openbookqa_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6c850a-08a1-4fde-8dea-37b59b4fe5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing dataset shards to the dataset hub:   0%|                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|████████████████████████████████████████████████| 2/2 [00:00<00:00, 473.61ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.30s/it]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\n",
      "Pushing dataset shards to the dataset hub:   0%|                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 1410.80ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "Pushing dataset shards to the dataset hub:   0%|                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.74ba/s]\u001b[A\n",
      "\n",
      "Upload 1 LFS files:   0%|                                                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\u001b[A\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████████████████████████████████████| 1/1 [00:01<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "openbookqa_chatml.push_to_hub(\"diwank/openbookqa_chatml\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e40bc-3f20-47bd-a74e-7e4d635cc427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
