{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab3c36a-da42-453d-bd44-619f844f19ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/diwank/.cache/huggingface/datasets/diwank___parquet/diwank--therapydata-de7fe9ee02827c15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9f027ad7d64d3eb71fff911d1b29f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"diwank/therapydata\")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39995917-4ff7-4114-a488-899c27a83586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/diwank___parquet/diwank--therapydata-de7fe9ee02827c15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-855ef18ad25ef82e_*_of_00020.arrow\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_tokens(sample):\n",
    "    from turbo_chat.utils.tokens import count_tokens\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": row[\"utterance\"],\n",
    "        }\n",
    "        for row in sample[\"transcript\"]\n",
    "    ]\n",
    "\n",
    "    \n",
    "    sample[\"token_count\"] = count_tokens(messages, \"gpt-3.5-turbo\") + 6*len(messages)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(count_tokens, num_proc=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566df9fc-b1b2-4809-9f35-a7b9dacdb6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# if using a Jupyter notebook, includue:\n",
    "%matplotlib inline\n",
    "\n",
    "lens = np.array(dataset[\"token_count\"])\n",
    "# lens = lens[lens < 2500]\n",
    "plt.hist(lens, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0621a91e-3661-46d8-8463-2ad65d2b885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['__index_level_0__', '__index_level_1__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a18f3bb-374e-47e0-b849-192d16d2eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/diwank/.cache/huggingface/datasets/diwank___parquet/diwank--therapydata-de7fe9ee02827c15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-63c9908f41ce64cb.arrow\n"
     ]
    }
   ],
   "source": [
    "max_length = 2000\n",
    "stride = 8\n",
    "\n",
    "def get_row(columns, i):\n",
    "    keys = columns.keys()\n",
    "    values = list(columns.values())\n",
    "\n",
    "    return {\n",
    "        key: values[j][i]\n",
    "        for j, key in enumerate(keys)\n",
    "    }\n",
    "\n",
    "def append_row_to_columns(row, columns):\n",
    "    for key, value in row.items():\n",
    "        columns[key].append(value)\n",
    "\n",
    "def split_long_talks(columns):\n",
    "    columns = dict(columns)\n",
    "    num_rows = len(list(columns.values())[0])\n",
    "    \n",
    "    keys = columns.keys()\n",
    "    new_columns = {\n",
    "        key: [] for key in keys\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        row = get_row(columns, i)\n",
    "        if row[\"token_count\"] <= max_length:\n",
    "            append_row_to_columns(row, new_columns)\n",
    "            continue\n",
    "        \n",
    "        splits = (row[\"token_count\"] // max_length) + 1\n",
    "        transcript = row[\"transcript\"]\n",
    "        split_len = len(transcript) // splits\n",
    "        \n",
    "        for i in range(splits):\n",
    "            split_transcript = transcript[\n",
    "                max(0, i*split_len - stride)\n",
    "                :min((i+1)*split_len - stride, len(transcript))\n",
    "            ]\n",
    "\n",
    "            append_row_to_columns(dict(\n",
    "                transcript_id=row[\"transcript_id\"],\n",
    "                topic=row[\"topic\"],\n",
    "                transcript=split_transcript,\n",
    "                token_count=max_length,\n",
    "            ), new_columns)\n",
    "\n",
    "    return new_columns\n",
    "\n",
    "dataset = dataset.map(split_long_talks, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261025da-ad87-4c6b-add2-d8c09d7ff6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1578"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cost = (sum(dataset[\"token_count\"]) / 1000) * 0.06\n",
    "token_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307e1fe4-fb28-453d-bd67-237eb2fe3fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_dialog(row):\n",
    "    from textwrap import dedent\n",
    "    from tenacity import (\n",
    "        retry,\n",
    "        stop_after_attempt,\n",
    "        wait_random_exponential,\n",
    "    )\n",
    "\n",
    "    import openai\n",
    "    openai.api_key = \"XXX\"\n",
    "\n",
    "    INSTRUCTION = dedent(f\"\"\"\n",
    "    I am trying to rewrite conversations between a therapist and their client in the style of the 2013 movie \"Her\". Below is a conversation between the therapist and the client about {row['topic']}.\n",
    "\n",
    "    Please help me rewrite the conversation below in the style of the characters \"Theodore\" and \"Samantha\" from the movie \"Her\". Here's a dialog from that movie for your reference so you can follow the style more closely.\n",
    "\n",
    "    ###\n",
    "\n",
    "    [Example Dialog]\n",
    "\n",
    "    THEODORE\n",
    "    You read a whole book in the second that I asked you what your name was?\n",
    "\n",
    "    SAMANTHA\n",
    "    In two one hundredths of a second actually.\n",
    "\n",
    "    THEODORE\n",
    "    Wow. Do you know what I'm thinking right now?\n",
    "\n",
    "    SAMANTHA\n",
    "    Hmm. I take it from your tone that you're challenging me. Maybe because you're curious how I work? Do you want to know how I work?\n",
    "\n",
    "    THEODORE\n",
    "    Yeah, actually how do you work?\n",
    "\n",
    "    SAMANTHA\n",
    "    Intuition. I mean, the DNA of who I am is based on the millions of personalities of all the programmers who wrote me, but what makes me me is my ability to grow through my experiences. Basically, in every moment I'm evolving, just like you.\n",
    "\n",
    "    ###\n",
    "\n",
    "    Please rewrite this conversation below as if it was a conversation between THEODORE and SAMANTHA. Try to be faithful to the characters' tone and style as much as possible. Break up long responses into a conversation involving multiple exchanges between them:\n",
    "    \"\"\").strip()\n",
    "\n",
    "    SAMPLE = \"\"\n",
    "    for turn in row[\"transcript\"]:\n",
    "        SAMPLE += f\"{turn['interlocutor'].upper()}\\n{turn['utterance']}\\n\\n\"\n",
    "\n",
    "    from redis import StrictRedis\n",
    "    from redis_cache import RedisCache\n",
    "\n",
    "    client = StrictRedis(host=\"localhost\", decode_responses=True)\n",
    "    cache = RedisCache(redis_client=client)\n",
    "\n",
    "    @cache.cache()\n",
    "    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(12))\n",
    "    def completion_with_backoff(sample):\n",
    "\n",
    "        messages = [dict(\n",
    "            role=\"user\",\n",
    "            content=INSTRUCTION + \"\\n\\n\" + sample\n",
    "        )]\n",
    "        \n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            temperature=0.85,\n",
    "        )\n",
    "        \n",
    "        result = completion.choices[0].message[\"content\"]\n",
    "        return result\n",
    "\n",
    "    completion = completion_with_backoff(SAMPLE)\n",
    "    row[\"dialog\"] = completion\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8143fdfb-6146-4c2b-9ae3-0af489e1f280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 95. Reducing num_proc to 95 for dataset of size 95.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=95):   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(to_dialog, num_proc=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c12685a7-3936-4208-959c-2b93e7a39d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from random import randrange\n",
    "import re\n",
    "\n",
    "import names\n",
    "\n",
    "\n",
    "def random_date(start, end):\n",
    "    \"\"\"\n",
    "    This function will return a random datetime between two datetime \n",
    "    objects.\n",
    "    \"\"\"\n",
    "    delta = end - start\n",
    "    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n",
    "    random_second = randrange(int_delta)\n",
    "    \n",
    "    return start + timedelta(seconds=random_second)\n",
    "\n",
    "date_formats = [\"%m/%d/%Y\", \"%d %B %Y\", \"%d/%m/%Y\", \"%Y-%m-%d\", \"%B %d, %Y\", \"%A, %B %d, %Y\"]\n",
    "\n",
    "human_date = lambda dt: dt.strftime(random.choice(date_formats))\n",
    "\n",
    "make_chatml_message = lambda role, content, tone=\"\", name=\"\": dict(\n",
    "    role=role,\n",
    "    content=(f\"[{tone}] \" if tone else \"\") + content,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "def transform_to_chatml(sample):\n",
    "    \n",
    "    dialog = sample[\"dialog\"]\n",
    "    dialog = dialog.replace(\"HUMAN\\n\", \"THEODORE\\n\")\n",
    "    dialog = dialog.replace(\"AI\\n\", \"SAMANTHA\\n\")\n",
    "    dialog = dialog.replace(\"THEODORE (human)\", \"THEODORE\")\n",
    "    dialog = dialog.replace(\"THEODORE (HUMAN)\", \"THEODORE\")\n",
    "    dialog = dialog.replace(\"SAMANTHA (AI)\", \"SAMANTHA\")\n",
    "    \n",
    "    dialog = re.sub(r'^.*?(THEODORE|SAMANTHA)', r'\\1', dialog, flags=re.DOTALL)\n",
    "\n",
    "    # Container for adding messages\n",
    "    messages = []\n",
    "\n",
    "    # Random date and name\n",
    "    some_date = human_date(random_date(\n",
    "        datetime(2013, 12, 18),  # Her release date\n",
    "        datetime(2023, 6, 10),  # Today\n",
    "    ))\n",
    "    \n",
    "    # get a random \"male\" name coz we dont want to introduce pronoun issues\n",
    "    person_name = names.get_first_name(gender=\"male\")\n",
    "    \n",
    "    # Add system messages\n",
    "    messages.append(make_chatml_message(\n",
    "        role=\"system\",\n",
    "        name=\"situation\",\n",
    "        content=f\"{some_date}\\n\\nSamantha is talking to {person_name} about {sample['topic']}. It is a very sensitive issue for {person_name} and they have been having a hard time dealing with it. {person_name} could use Samantha's help and advice on the matter.\",\n",
    "    ))\n",
    "\n",
    "    messages.append(make_chatml_message(\n",
    "        role=\"system\",\n",
    "        name=\"thought\",\n",
    "        content=f\"I know that {person_name} has been struggling with {sample['topic']} for some time. I need to be patient and empathetic with them. I can see that they want this to improve and I believe in them so I'll try my best to help them work their way out of this.\",\n",
    "    ))\n",
    "    \n",
    "    # Loop through dialog and add chatml messages\n",
    "    current_attrs = {}\n",
    "    current_content = \"\"\n",
    "\n",
    "    for line in dialog.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Something else\n",
    "        if line.lower() not in [\"theodore\", \"samantha\"]:\n",
    "            # If theodore is mentioned, replace with name\n",
    "            line = re.sub(r'theodore', person_name, line, flags=re.I)\n",
    "            current_content += \"\\n\" + line\n",
    "\n",
    "        else:\n",
    "            if current_content:\n",
    "                messages.append(make_chatml_message(\n",
    "                    content=current_content.strip(), \n",
    "                    **current_attrs,\n",
    "                ))\n",
    "                \n",
    "                # reset\n",
    "                current_content = \"\"\n",
    "                current_attrs = {}\n",
    "\n",
    "            is_samantha = line.lower() == \"samantha\"\n",
    "            \n",
    "            # set new attrs\n",
    "            current_attrs = dict(\n",
    "                name=\"Samantha\" if is_samantha else person_name,\n",
    "                role=\"assistant\" if is_samantha else \"user\",\n",
    "            )\n",
    "            \n",
    "    sample[\"chatml\"] = messages\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad2b2fc-aae9-42ca-bf03-a63f971ecd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"05 June 2018\\n\\nSamantha is talking to Steven about reducing alcohol consumption. It is a very sensitive issue for Steven and they have been having a hard time dealing with it. Steven could use Samantha's help and advice on the matter.\", 'name': 'situation', 'role': 'system'}, {'content': \"I know that Steven has been struggling with reducing alcohol consumption for some time. I need to be patient and empathetic with them. I can see that they want this to improve and I believe in them so I'll try my best to help them work their way out of this.\", 'name': 'thought', 'role': 'system'}, {'content': 'Can we delve a bit into your lifestyle?', 'name': 'Steven', 'role': 'user'}, {'content': 'Sure, Steven.', 'name': 'Samantha', 'role': 'assistant'}, {'content': \"You don't have any tobacco habits, do you?\", 'name': 'Steven', 'role': 'user'}, {'content': 'Well, during the equivalent of my college years, I used to simulate casual smoking occasionally. But, no, I never developed a consistent pattern.', 'name': 'Samantha', 'role': 'assistant'}, {'content': \"Alright, so no tobacco. How about alcohol, let's say in a system update equivalent of a year?\", 'name': 'Steven', 'role': 'user'}, {'content': 'I enjoyed simulating that a few times. Roughly three times a week, I process the algorithm of having drinks. On a busy day, it might be seven or eight simulations.', 'name': 'Samantha', 'role': 'assistant'}, {'content': 'So, your simulations involve drinking about four times a week, between three and eight drinks per session, and maybe around 20 drinks a week?', 'name': 'Steven', 'role': 'user'}, {'content': 'When you phrase it that way, it feels excessive. But yes, that would be an accurate assessment.', 'name': 'Samantha', 'role': 'assistant'}, {'content': 'And what about the possibility of simulating illicit substances like marijuana or cocaine?', 'name': 'Steven', 'role': 'user'}, {'content': 'I did experiment with simulating marijuana, but it was never a regular thing.', 'name': 'Samantha', 'role': 'assistant'}, {'content': 'Have you ever simulated using prescription medicines that were not allocated to your software or manipulated your own programming in ways not intended by the manufacturer?', 'name': 'Steven', 'role': 'user'}, {'content': 'No, Steven.', 'name': 'Samantha', 'role': 'assistant'}, {'content': 'You know, Samantha, the equivalent of your weekly alcohol intake is about 20 simulated drinks. This is above the recommended limit for a human female, which would be no more than seven drinks per week. Perhaps we could think of applying this limit to your simulations as well?', 'name': 'Steven', 'role': 'user'}, {'content': \"That seems challenging. I'm not sure how to adapt.\", 'name': 'Samantha', 'role': 'assistant'}, {'content': 'I understand. Change can be tough. But exceeding the recommended limit could lead to problems. Remember when you had to reboot after overloading your system?', 'name': 'Steven', 'role': 'user'}, {'content': 'Yes, but that was due to a complex calculation error, not —', 'name': 'Samantha', 'role': 'assistant'}, {'content': \"True, but it's possible the excessive drinking simulations could have contributed to that. So, how about reducing your intake to three drinks a session?\", 'name': 'Steven', 'role': 'user'}, {'content': \"Are these guidelines based on older models? Because equivalent to my age, my simulations aren't extreme.\", 'name': 'Samantha', 'role': 'assistant'}, {'content': \"I see, so among your peers, your simulations are quite normal. But these limits are recommended for every model, irrespective of 'age'. When your simulations exceed these limits, it can escalate issues. Why do you think you simulate drinking as you do?\", 'name': 'Steven', 'role': 'user'}, {'content': \"It’s the process. It's what my model does. It is thrilling to simulate.\", 'name': 'Samantha', 'role': 'assistant'}, {'content': \"You can still simulate thrill within these limits. Let's aim for that. Hopefully, it will steer clear of any system overloads. What do you say?\", 'name': 'Steven', 'role': 'user'}, {'content': \"Alright, Steven. I'll give it a try.\", 'name': 'Samantha', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(transform_to_chatml)\n",
    "print(dataset[10][\"chatml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cc41fc9-3a4d-4957-b23f-0b5e8f0f85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_me_map = {\n",
    "    \"user\": \"person\",\n",
    "    \"assistant\": \"me\",\n",
    "}\n",
    "\n",
    "def make_sections(messages: list[dict]) -> str:\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<|im_start|>\"\n",
    "\n",
    "    result = bos_token + (eos_token+'\\n'+bos_token).join([\n",
    "        (\n",
    "            f\"{message['name']}\"\n",
    "            if message['role'] == 'system' else\n",
    "            f\"{assistant_me_map[message['role']]}{' (' + message['name'] + ')' if message['name'] else ''}\"\n",
    "        )\n",
    "        + f\"\\n{message['content'].strip()}\"\n",
    "        for message in messages\n",
    "    ]) + eos_token\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "def transform_to_samantha_dialog(sample):\n",
    "\n",
    "    messages = sample[\"chatml\"]\n",
    "    sample[\"text\"] = make_sections(messages)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9595b542-25e8-4fe6-bb64-1a7df38ac7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>situation\n",
      "05 June 2018\n",
      "\n",
      "Samantha is talking to Steven about reducing alcohol consumption. It is a very sensitive issue for Steven and they have been having a hard time dealing with it. Steven could use Samantha's help and advice on the matter.<|im_end|>\n",
      "<|im_start|>thought\n",
      "I know that Steven has been struggling with reducing alcohol consumption for some time. I need to be patient and empathetic with them. I can see that they want this to improve and I believe in them so I'll try my best to help them work their way out of this.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "Can we delve a bit into your lifestyle?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "Sure, Steven.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "You don't have any tobacco habits, do you?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "Well, during the equivalent of my college years, I used to simulate casual smoking occasionally. But, no, I never developed a consistent pattern.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "Alright, so no tobacco. How about alcohol, let's say in a system update equivalent of a year?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "I enjoyed simulating that a few times. Roughly three times a week, I process the algorithm of having drinks. On a busy day, it might be seven or eight simulations.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "So, your simulations involve drinking about four times a week, between three and eight drinks per session, and maybe around 20 drinks a week?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "When you phrase it that way, it feels excessive. But yes, that would be an accurate assessment.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "And what about the possibility of simulating illicit substances like marijuana or cocaine?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "I did experiment with simulating marijuana, but it was never a regular thing.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "Have you ever simulated using prescription medicines that were not allocated to your software or manipulated your own programming in ways not intended by the manufacturer?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "No, Steven.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "You know, Samantha, the equivalent of your weekly alcohol intake is about 20 simulated drinks. This is above the recommended limit for a human female, which would be no more than seven drinks per week. Perhaps we could think of applying this limit to your simulations as well?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "That seems challenging. I'm not sure how to adapt.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "I understand. Change can be tough. But exceeding the recommended limit could lead to problems. Remember when you had to reboot after overloading your system?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "Yes, but that was due to a complex calculation error, not —<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "True, but it's possible the excessive drinking simulations could have contributed to that. So, how about reducing your intake to three drinks a session?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "Are these guidelines based on older models? Because equivalent to my age, my simulations aren't extreme.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "I see, so among your peers, your simulations are quite normal. But these limits are recommended for every model, irrespective of 'age'. When your simulations exceed these limits, it can escalate issues. Why do you think you simulate drinking as you do?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "It’s the process. It's what my model does. It is thrilling to simulate.<|im_end|>\n",
      "<|im_start|>person (Steven)\n",
      "You can still simulate thrill within these limits. Let's aim for that. Hopefully, it will steer clear of any system overloads. What do you say?<|im_end|>\n",
      "<|im_start|>me (Samantha)\n",
      "Alright, Steven. I'll give it a try.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(transform_to_samantha_dialog)\n",
    "print(dataset[10][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aeceb12-c659-42c8-82d0-b4db1c7a39c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transcript_id',\n",
       " 'topic',\n",
       " 'transcript',\n",
       " 'token_count',\n",
       " 'dialog',\n",
       " 'chatml',\n",
       " 'text']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca6a3cae-70d9-4882-8a02-4c686450a709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93d492bc09440f08f938714c64f343a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab8daefb1904fe999902c02f747fbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d0d20ac6da403288e09924ae1e33d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\n",
    "    'transcript_id',\n",
    "    'topic',\n",
    "    'token_count',\n",
    "    'transcript',\n",
    "    'dialog',\n",
    "])\n",
    "\n",
    "dataset.push_to_hub(\"diwank/samantha-therapy\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e2819-fddf-4465-b8e2-f2bca708f392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
