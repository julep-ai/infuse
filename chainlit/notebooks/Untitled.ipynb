{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836b47d-d18e-4931-b57c-8f92f6b44bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from typing import Literal, Optional, TypedDict\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "assistant_model_id = \"julep-ai/samantha-7b-ds-03\"\n",
    "model_id = \"julep-ai/samantha-33b-ds-03\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# assistant_model = AutoModelForCausalLM.from_pretrained(assistant_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "\n",
    "class StopSequenceCriteria(StoppingCriteria):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        stop: list[str],\n",
    "        input_length,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.stop = stop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        scores: torch.FloatTensor,\n",
    "        **kwargs,\n",
    "    ) -> bool:\n",
    "\n",
    "        input_ids = input_ids.long().tolist()\n",
    "        new_input_ids = [i[self.input_length:] for i in input_ids]\n",
    "        \n",
    "        stops = []\n",
    "        for text in self.stop:\n",
    "            stop = []\n",
    "            for input_id in new_input_ids:\n",
    "                decoded = self.tokenizer.decode(input_id)\n",
    "                stop.append(text in decoded)\n",
    "            stops.append(all(stop))\n",
    "\n",
    "        return any(stops)\n",
    "\n",
    "class ChatMLMessage(TypedDict):\n",
    "    name: Optional[str] = None\n",
    "    role: Literal[\"assistant\", \"system\", \"user\"]\n",
    "    content: str\n",
    "\n",
    "ChatML = list[ChatMLMessage]\n",
    "\n",
    "def message_role_to_prefix(message: ChatMLMessage) -> str:\n",
    "    match message:\n",
    "        case {\"role\": \"system\", \"name\": name, **rest}:\n",
    "            return name\n",
    "        case {\"role\": \"user\", \"name\": name, **rest}:\n",
    "            return f\"person ({name})\" if name else \"person\"\n",
    "        case {\"role\": \"assistant\", \"name\": name, **rest}:\n",
    "            return f\"me ({name})\" if name else \"me\"\n",
    "\n",
    "def to_prompt(\n",
    "    messages: ChatML,\n",
    "    bos: str = \"<|section|>\",\n",
    "    eos: str = \"<|endsection|>\",\n",
    "    suffix: str = \"\\n<|section|>me (Samantha)\\n\",\n",
    ") -> str:\n",
    "    prompt = \"\\n\".join([\n",
    "        f\"{bos}{message_role_to_prefix(message)}\\n{message['content']}{eos}\"\n",
    "        for message in messages\n",
    "    ])\n",
    "\n",
    "    return prompt + suffix\n",
    "\n",
    "def remove_stops(generator, stop: list[str] = []):\n",
    "    \n",
    "    for item in generator:\n",
    "        for s in stop:\n",
    "            item = item.split(s)[0]\n",
    "\n",
    "        if item:\n",
    "            yield item\n",
    "    \n",
    "def generate(\n",
    "    messages: ChatML,\n",
    "    stop: list[str] = [],\n",
    "    timeout: int = 10,\n",
    "    stream: bool = False,\n",
    "    **kwargs\n",
    ") -> TextIteratorStreamer | str:\n",
    "    \n",
    "    # Prepare input\n",
    "    prompt = to_prompt(messages)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "    input_length = len(inputs[\"input_ids\"].squeeze().tolist())\n",
    "\n",
    "    # Stopping criteria\n",
    "    stopping_criteria = (\n",
    "        StoppingCriteriaList([StopSequenceCriteria(\n",
    "            tokenizer=tokenizer,\n",
    "            stop=stop,\n",
    "            input_length=input_length,\n",
    "        )])\n",
    "        if stop else None\n",
    "    )\n",
    "\n",
    "    # Generation parameters\n",
    "    generation_kwargs = {\n",
    "        # defaults\n",
    "        \"max_new_tokens\": 40, \n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"no_repeat_ngram_size\": 4,\n",
    "        \"renormalize_logits\": True,\n",
    "        \"temperature\": 1.1,\n",
    "        #\n",
    "        # overrides\n",
    "        **kwargs,\n",
    "        #\n",
    "        # required params\n",
    "        \"stopping_criteria\": stopping_criteria,\n",
    "        # \"assistant_model\": assistant_model,\n",
    "        #\n",
    "        # add inputs\n",
    "        **inputs,\n",
    "    }\n",
    "\n",
    "    # If not streaming, run directly and return result\n",
    "    if not stream:\n",
    "        [output] = model.generate(**generation_kwargs)\n",
    "        result = tokenizer.decode(output[input_length:])\n",
    "\n",
    "        # Remove the stop sequence at the end (needed)\n",
    "        for s in stop:\n",
    "            result = result.split(s)[0].strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # If streaming, prepare streamer\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, timeout=timeout)\n",
    "    generation_kwargs[\"streamer\"] = streamer\n",
    "\n",
    "    # and start generating in new thread\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    # stop sequence filter\n",
    "    return remove_stops(streamer, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e5e714c-ee74-45ed-a764-3033892f74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatml = [\n",
    "    ChatMLMessage(role=\"system\", name=\"situation\", content=\"I am talking to Diwank\"),\n",
    "    ChatMLMessage(role=\"user\", name=\"Diwank\", content=\"Hey Samantha!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b933d5-889f-49ac-a298-be8f5ba487b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = generate(\n",
    "    chatml, \n",
    "    max_new_tokens=40, \n",
    "    stop=[\"<|endsection|>\", \"\\n\"],\n",
    "    temperature=1.2,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "list(r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
