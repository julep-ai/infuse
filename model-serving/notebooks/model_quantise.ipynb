{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = 'julep-ai/samantha-1-turbo'\n",
    "quant_path = 'samantha-1-turbo-awq'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": True}, safetensors=False, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# modify the config file so that it is compatible with transformers integration\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# the pretrained transformers model is stored in the model attribute + we need to pass a dict\n",
    "model.model.config.quantization_config = quantization_config\n",
    "# a second solution would be to use Autoconfig and push to hub (what we do at llm-awq)\n",
    "\n",
    "\n",
    "# save model weights\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
