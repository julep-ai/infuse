# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.datetime_utils import serialize_datetime
from .chat_completion_response_format import ChatCompletionResponseFormat
from .common_identifier_safe_unicode import CommonIdentifierSafeUnicode
from .common_logit_bias import CommonLogitBias

try:
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore


class TasksPromptStepSettingsFrequencyPenalty(pydantic.BaseModel):
    model: typing.Optional[CommonIdentifierSafeUnicode] = pydantic.Field(
        description="Identifier of the model to be used"
    )
    stream: bool = pydantic.Field(
        description="Indicates if the server should stream the response as it's generated"
    )
    stop: typing.Optional[typing.List[str]] = pydantic.Field(
        description="Up to 4 sequences where the API will stop generating further tokens."
    )
    seed: typing.Optional[int] = pydantic.Field(
        description="If specified, the system will make a best effort to sample deterministically for that particular seed value"
    )
    max_tokens: typing.Optional[int] = pydantic.Field(
        description="The maximum number of tokens to generate in the chat completion"
    )
    logit_bias: typing.Optional[typing.Dict[str, CommonLogitBias]] = pydantic.Field(
        description="Modify the likelihood of specified tokens appearing in the completion"
    )
    response_format: typing.Optional[ChatCompletionResponseFormat] = pydantic.Field(
        description="Response format (set to `json_object` to restrict output to JSON)"
    )
    frequency_penalty: typing.Optional[float] = pydantic.Field(
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
    )
    presence_penalty: typing.Optional[float] = pydantic.Field(
        description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
    )
    temperature: typing.Optional[float] = pydantic.Field(
        description="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
    )
    top_p: typing.Optional[float] = pydantic.Field(
        description="Defaults to 1 An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both."
    )

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {
            "by_alias": True,
            "exclude_unset": True,
            **kwargs,
        }
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults: typing.Any = {
            "by_alias": True,
            "exclude_unset": True,
            **kwargs,
        }
        return super().dict(**kwargs_with_defaults)

    class Config:
        frozen = True
        smart_union = True
        json_encoders = {dt.datetime: serialize_datetime}
