{
    "openapi": "3.1.0",
    "info": {
        "title": "FastAPI",
        "version": "0.1.0"
    },
    "paths": {
        "/v1/models": {
            "get": {
                "summary": "Show Available Models",
                "description": "Show available models. Right now we only have one model.",
                "operationId": "show_available_models_v1_models_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/v1/chat/completions": {
            "post": {
                "summary": "Create Chat Completion",
                "description": "Completion API similar to OpenAI's API.\n\nSee  https://platform.openai.com/docs/api-reference/chat/create\nfor the API specification. This API mimics the OpenAI ChatCompletion API.\n\nNOTE: Currently we do not support the following features:\n    - function_call (Users should implement this by themselves)\n    - logit_bias (to be supported by vLLM engine)",
                "operationId": "create_chat_completion_v1_chat_completions_post",
                "parameters": [
                    {
                        "name": "model",
                        "in": "query",
                        "required": true,
                        "schema": {
                            "type": "string"
                        },
                        "description": "ID of the model to use. Indicates the model endpoint compatibility."
                    },
                    {
                        "name": "messages",
                        "in": "body",
                        "required": true,
                        "schema": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "role": {
                                        "type": "string",
                                        "enum": [
                                            "system",
                                            "user",
                                            "assistant",
                                            "function"
                                        ]
                                    },
                                    "content": {
                                        "type": "string"
                                    },
                                    "name": {
                                        "type": "string"
                                    },
                                    "function_call": {
                                        "type": [
                                            "string",
                                            "object"
                                        ]
                                    }
                                },
                                "required": [
                                    "role",
                                    "content"
                                ]
                            }
                        },
                        "description": "A list of messages comprising the conversation so far."
                    },
                    {
                        "name": "temperature",
                        "in": "query",
                        "schema": {
                            "type": "number",
                            "default": 1
                        },
                        "description": "Sampling temperature to use, between 0 and 2. Higher values like 0.8 make the output more random, while lower values like 0.2 make it more focused and deterministic."
                    },
                    {
                        "name": "top_p",
                        "in": "query",
                        "schema": {
                            "type": "number",
                            "default": 1
                        },
                        "description": "An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass."
                    },
                    {
                        "name": "n",
                        "in": "query",
                        "schema": {
                            "type": "integer",
                            "default": 1
                        },
                        "description": "How many chat completion choices to generate for each input message."
                    },
                    {
                        "name": "stream",
                        "in": "query",
                        "schema": {
                            "type": "boolean",
                            "default": false
                        },
                        "description": "If set, partial message deltas will be sent as server-sent events, like in ChatGPT."
                    },
                    {
                        "name": "stop",
                        "in": "query",
                        "schema": {
                            "type": [
                                "string",
                                "array"
                            ],
                            "default": null
                        },
                        "description": "Up to 4 sequences where the API will stop generating further tokens."
                    },
                    {
                        "name": "max_tokens",
                        "in": "query",
                        "schema": {
                            "type": "integer",
                            "default": "inf"
                        },
                        "description": "The maximum number of tokens to generate in the chat completion."
                    },
                    {
                        "name": "presence_penalty",
                        "in": "query",
                        "schema": {
                            "type": "number",
                            "default": 0
                        },
                        "description": "A number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
                    },
                    {
                        "name": "frequency_penalty",
                        "in": "query",
                        "schema": {
                            "type": "number",
                            "default": 0
                        },
                        "description": "A number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
                    },
                    {
                        "name": "logit_bias",
                        "in": "query",
                        "schema": {
                            "type": "object",
                            "default": null
                        },
                        "description": "Modify the likelihood of specified tokens appearing in the completion."
                    },
                    {
                        "name": "user",
                        "in": "query",
                        "schema": {
                            "type": "string"
                        },
                        "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more."
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/v1/completions": {
            "post": {
                "summary": "Create Completion",
                "description": "Completion API similar to OpenAI's API.\n\nSee https://platform.openai.com/docs/api-reference/completions/create\nfor the API specification. This API mimics the OpenAI Completion API.\n\nNOTE: Currently we do not support the following features:\n    - echo (since the vLLM engine does not currently support\n      getting the logprobs of prompt tokens)\n    - suffix (the language models we currently support do not support\n      suffix)\n    - logit_bias (to be supported by vLLM engine)",
                "operationId": "create_completion_v1_completions_post",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        }
    }
}
