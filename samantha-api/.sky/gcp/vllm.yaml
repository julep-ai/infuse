name: samantha-1-turbo-vllm

resources:
  cloud: gcp
  # Specify accelerators as a list (which defines priority)
  # This is descending order of throughput/$
  accelerators: ["A100-80:1", "A100:1", "A100-40:1", "L4:1"]
  use_spot: true
  disk_size: 256
  disk_tier: high
  ports: 8000

service:
  readiness_probe:
    path: /status
    initial_delay_seconds: 1200

  replica_policy: # All required for autoscaling
    min_replicas: 2
    max_replicas: 9
    target_qps_per_replica: 10

envs:
  MODEL_NAME: julep-ai/samantha-1-turbo
  REVISION: b5b76dee2d0002f5f5a778e2857bd846cc63cf42
  SENTRY_DSN: "SET_MANUALLY"
  HUGGING_FACE_HUB_TOKEN: "SET_MANUALLY"
  HF_TOKEN: "SET_MANUALLY"
  BACKLOG: "4096"
  PORT: "8000"
  HOST: "0.0.0.0"
  BLOCK_SIZE: "8"
  MAX_MODEL_LEN: "16384"

workdir: . # Project home

setup: |
  conda activate samantha-api
  if [ $? -ne 0 ]; then
    conda create -n samantha-api -c conda-forge python=3.11 poetry cxx-compiler -y
    conda activate samantha-api
    conda install -c "nvidia/label/cuda-12.3.2" -y cuda-toolkit cuda-cudart-dev libcublas-dev libcusparse-dev libcusolver-dev cuda-nvml-dev
    conda install -c conda-forge -y cudnn
  fi

  # Install dependencies
  export PATH="~/.conda/include:~/.conda/envs/samantha-api/include:$PATH"
  export CPATH="~/.conda/include:~/.conda/envs/samantha-api/include:$CPATH"
  export C_INCLUDE_PATH="~/.conda/include:~/.conda/envs/samantha-api/include:$C_INCLUDE_PATH"
  export LD_LIBRARY_PATH="~/.conda/lib:~/.conda/envs/samantha-api/lib:$LD_LIBRARY_PATH"
  poetry config virtualenvs.create false
  poetry install --directory services/vllm

run: |
  conda activate samantha-api

  echo "Starting service"

  python samantha_api/web.py \
    --model $MODEL_NAME \
    --tokenizer $MODEL_NAME \
    --revision $REVISION \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --port $PORT \
    --backlog $BACKLOG \
    --block-size $BLOCK_SIZE \
    --trust-remote-code \
    --max-model-len $MAX_MODEL_LEN
