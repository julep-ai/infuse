# generated by datamodel-codegen:
#   filename:  openapi-0.4.0.yaml

from __future__ import annotations

from typing import Annotated, Literal
from uuid import UUID

from pydantic import AwareDatetime, BaseModel, ConfigDict, Field, RootModel

from .Docs import DocReference
from .Entries import ChatMLMessage


class BaseChatOutput(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    index: int
    finish_reason: Literal["stop", "length", "content_filter", "tool_calls"]
    """
    The reason the model stopped generating tokens
    """
    logprobs: Annotated[LogProbResponse | None, Field(...)]
    """
    The log probabilities of tokens
    """


class BaseChatResponse(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    usage: Annotated[CompetionUsage | None, Field(...)]
    """
    Usage statistics for the completion request
    """
    jobs: list[UUID]
    """
    Background job IDs that may have been spawned from this interaction.
    """
    docs: list[DocReference]
    """
    Documents referenced for this request (for citation purposes).
    """
    created_at: Annotated[AwareDatetime, Field(json_schema_extra={"readOnly": True})]
    """
    When this resource was created as UTC date-time
    """
    id: Annotated[UUID, Field(json_schema_extra={"readOnly": True})]


class BaseTokenLogProb(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    token: str
    logprob: float
    """
    The log probability of the token
    """
    bytes: Annotated[list[int] | None, Field(...)]


class ChatOutputChunk(BaseChatOutput):
    """
    Streaming chat completion output
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    delta: ChatMLMessage
    """
    The message generated by the model
    """


class ChunkChatResponse(BaseChatResponse):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    choices: list[ChatOutputChunk]
    """
    The deltas generated by the model
    """


class CompetionUsage(BaseModel):
    """
    Usage statistics for the completion request
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    completion_tokens: Annotated[int, Field(json_schema_extra={"readOnly": True})]
    """
    Number of tokens in the generated completion
    """
    prompt_tokens: Annotated[int, Field(json_schema_extra={"readOnly": True})]
    """
    Number of tokens in the prompt
    """
    total_tokens: Annotated[int, Field(json_schema_extra={"readOnly": True})]
    """
    Total number of tokens used in the request (prompt + completion)
    """


class CompletionResponseFormat(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    type: Literal["text", "json_object"] = "text"
    """
    The format of the response
    """


class LogProbResponse(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    content: Annotated[list[TokenLogProb] | None, Field(...)]
    """
    The log probabilities of the tokens
    """


class MessageChatResponse(ChunkChatResponse):
    pass


class MultipleChatOutput(BaseChatOutput):
    """
    The output returned by the model. Note that, depending on the model provider, they might return more than one message.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    messages: list[ChatMLMessage]


class SingleChatOutput(BaseChatOutput):
    """
    The output returned by the model. Note that, depending on the model provider, they might return more than one message.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    message: ChatMLMessage


class TokenLogProb(BaseTokenLogProb):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    top_logprobs: list[BaseTokenLogProb]
